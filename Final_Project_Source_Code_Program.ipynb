{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Merha23/AI_Tigrinya_Translation/blob/main/Final_Project_Source_Code_Program.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "      **Integration GitHub with Google Colab**"
      ],
      "metadata": {
        "id": "I2mjbZgs5RLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the GitHub Repository in Google Colab\n",
        "# Authenticate Google Colab with GitHub\n",
        "# Clone with Authentication\n",
        "# Generate a GitHub Personal Access Token (PAT)\n",
        "# How to Use Your GitHub Token in Google Colab Securely\n",
        "# cloning your repository using the token:\n",
        "# Use the stored token to clone your GitHub repository in Google Colab:\n",
        "# Clone the Repository Securely in Colab\n",
        "# Instead of using your username/password, use the token as follows:\n",
        "\n",
        "# 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   Personal Access Token (PAT)\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "token = getpass('Enter your GitHub Personal Access Token: ')\n",
        "os.environ[\"GITHUB_TOKEN\"] = token\n",
        "\n",
        "repo_url = f\"https://{token}@github.com/Merha23/AI_Tigrinya_Translation.git\"\n",
        "!git clone {repo_url}\n",
        "%cd AI_Tigrinya_Translation\n"
      ],
      "metadata": {
        "id": "MN7ZR_4k6veK",
        "outputId": "5f265e67-57cb-4cc1-f56b-bf541034a834",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GitHub Personal Access Token: ··········\n",
            "Cloning into 'AI_Tigrinya_Translation'...\n",
            "remote: Enumerating objects: 168, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "Receiving objects: 100% (168/168), 901.44 KiB | 7.64 MiB/s, done.\n",
            "remote: Total 168 (delta 57), reused 41 (delta 41), pack-reused 100 (from 1)\u001b[K\n",
            "Resolving deltas: 100% (99/99), done.\n",
            "/content/AI_Tigrinya_Translation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf .git\n",
        "!git init\n"
      ],
      "metadata": {
        "id": "HLmPia8hkGQv",
        "outputId": "ae533b36-ca7c-4a3e-832d-aa0a85f26851",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/AI_Tigrinya_Translation/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/AI_Tigrinya_Translation\n"
      ],
      "metadata": {
        "id": "jgj4YCfYkYXr",
        "outputId": "c2fbe3b4-597d-4c14-de47-04b75f299024",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AI_Tigrinya_Translation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this python code and if cloning is successful, it should display the repository contents.\n",
        "\n",
        "!ls -la\n"
      ],
      "metadata": {
        "id": "LehnBNoMKIQA",
        "outputId": "34d01173-60a3-41f4-8177-cbb5f1160f74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1100\n",
            "drwxr-xr-x 4 root root   4096 Apr  7 07:48  .\n",
            "drwxr-xr-x 1 root root   4096 Apr  7 07:48  ..\n",
            "-rw-r--r-- 1 root root   6607 Apr  7 07:48  Cheat_Sheet_Python_Code.ipynb\n",
            "drwxr-xr-x 2 root root   4096 Apr  7 07:48 'Cleaned dataset'\n",
            "-rw-r--r-- 1 root root 274619 Apr  7 07:48 'cleaned_Medical Translation.csv'\n",
            "-rw-r--r-- 1 root root  78440 Apr  7 07:48  Final_Project_Source_Code_Program.ipynb\n",
            "drwxr-xr-x 7 root root   4096 Apr  7 07:48  .git\n",
            "-rw-r--r-- 1 root root   3415 Apr  7 07:48  .gitignore\n",
            "-rw-r--r-- 1 root root   1064 Apr  7 07:48  LICENSE\n",
            "-rw-r--r-- 1 root root 146757 Apr  7 07:48 'Medical Translation.csv'\n",
            "-rw-r--r-- 1 root root   1460 Apr  7 07:48  Practices.ipynb\n",
            "-rw-r--r-- 1 root root 574859 Apr  7 07:48 'Proposal for Final Project.pdf'\n",
            "-rw-r--r-- 1 root root    103 Apr  7 07:48  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive in Colab\n",
        "# Run this command in Colab to access your Drive:\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "M9JMtl6s95zw",
        "outputId": "48cb9458-e9b6-4e0d-9597-75c2ae386fec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull the Latest Changes from GitHub\n",
        "# Before pushing your changes, you need to sync your local copy of the repository with the remote one.\n",
        "# Pull the latest changes:\n",
        "# Run the following command to fetch and merge the latest changes from GitHub into your local branch:\n",
        "\n",
        "!git pull https://{os.environ['GITHUB_TOKEN']}@github.com/Merha23/AI_Tigrinya_Translation.git main"
      ],
      "metadata": {
        "id": "z_N_oHc90qRA",
        "outputId": "9975ef4c-078d-4b9a-a34d-19d10a9a7d70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 168, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/68)\u001b[K\rremote: Counting objects:   2% (2/68)\u001b[K\rremote: Counting objects:   4% (3/68)\u001b[K\rremote: Counting objects:   5% (4/68)\u001b[K\rremote: Counting objects:   7% (5/68)\u001b[K\rremote: Counting objects:   8% (6/68)\u001b[K\rremote: Counting objects:  10% (7/68)\u001b[K\rremote: Counting objects:  11% (8/68)\u001b[K\rremote: Counting objects:  13% (9/68)\u001b[K\rremote: Counting objects:  14% (10/68)\u001b[K\rremote: Counting objects:  16% (11/68)\u001b[K\rremote: Counting objects:  17% (12/68)\u001b[K\rremote: Counting objects:  19% (13/68)\u001b[K\rremote: Counting objects:  20% (14/68)\u001b[K\rremote: Counting objects:  22% (15/68)\u001b[K\rremote: Counting objects:  23% (16/68)\u001b[K\rremote: Counting objects:  25% (17/68)\u001b[K\rremote: Counting objects:  26% (18/68)\u001b[K\rremote: Counting objects:  27% (19/68)\u001b[K\rremote: Counting objects:  29% (20/68)\u001b[K\rremote: Counting objects:  30% (21/68)\u001b[K\rremote: Counting objects:  32% (22/68)\u001b[K\rremote: Counting objects:  33% (23/68)\u001b[K\rremote: Counting objects:  35% (24/68)\u001b[K\rremote: Counting objects:  36% (25/68)\u001b[K\rremote: Counting objects:  38% (26/68)\u001b[K\rremote: Counting objects:  39% (27/68)\u001b[K\rremote: Counting objects:  41% (28/68)\u001b[K\rremote: Counting objects:  42% (29/68)\u001b[K\rremote: Counting objects:  44% (30/68)\u001b[K\rremote: Counting objects:  45% (31/68)\u001b[K\rremote: Counting objects:  47% (32/68)\u001b[K\rremote: Counting objects:  48% (33/68)\u001b[K\rremote: Counting objects:  50% (34/68)\u001b[K\rremote: Counting objects:  51% (35/68)\u001b[K\rremote: Counting objects:  52% (36/68)\u001b[K\rremote: Counting objects:  54% (37/68)\u001b[K\rremote: Counting objects:  55% (38/68)\u001b[K\rremote: Counting objects:  57% (39/68)\u001b[K\rremote: Counting objects:  58% (40/68)\u001b[K\rremote: Counting objects:  60% (41/68)\u001b[K\rremote: Counting objects:  61% (42/68)\u001b[K\rremote: Counting objects:  63% (43/68)\u001b[K\rremote: Counting objects:  64% (44/68)\u001b[K\rremote: Counting objects:  66% (45/68)\u001b[K\rremote: Counting objects:  67% (46/68)\u001b[K\rremote: Counting objects:  69% (47/68)\u001b[K\rremote: Counting objects:  70% (48/68)\u001b[K\rremote: Counting objects:  72% (49/68)\u001b[K\rremote: Counting objects:  73% (50/68)\u001b[K\rremote: Counting objects:  75% (51/68)\u001b[K\rremote: Counting objects:  76% (52/68)\u001b[K\rremote: Counting objects:  77% (53/68)\u001b[K\rremote: Counting objects:  79% (54/68)\u001b[K\rremote: Counting objects:  80% (55/68)\u001b[K\rremote: Counting objects:  82% (56/68)\u001b[K\rremote: Counting objects:  83% (57/68)\u001b[K\rremote: Counting objects:  85% (58/68)\u001b[K\rremote: Counting objects:  86% (59/68)\u001b[K\rremote: Counting objects:  88% (60/68)\u001b[K\rremote: Counting objects:  89% (61/68)\u001b[K\rremote: Counting objects:  91% (62/68)\u001b[K\rremote: Counting objects:  92% (63/68)\u001b[K\rremote: Counting objects:  94% (64/68)\u001b[K\rremote: Counting objects:  95% (65/68)\u001b[K\rremote: Counting objects:  97% (66/68)\u001b[K\rremote: Counting objects:  98% (67/68)\u001b[K\rremote: Counting objects: 100% (68/68)\u001b[K\rremote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 168 (delta 57), reused 41 (delta 41), pack-reused 100 (from 1)\u001b[K\n",
            "Receiving objects: 100% (168/168), 901.44 KiB | 9.91 MiB/s, done.\n",
            "Resolving deltas: 100% (99/99), done.\n",
            "From https://github.com/Merha23/AI_Tigrinya_Translation\n",
            " * branch            main       -> FETCH_HEAD\n",
            "error: The following untracked working tree files would be overwritten by merge:\n",
            "\t.gitignore\n",
            "\tCheat_Sheet_Python_Code.ipynb\n",
            "\tCleaned dataset/Train and test.md\n",
            "\tCleaned dataset/cleaned_Medical Translation (5).csv\n",
            "\tFinal_Project_Source_Code_Program.ipynb\n",
            "\tLICENSE\n",
            "\tMedical Translation.csv\n",
            "\tPractices.ipynb\n",
            "\tProposal for Final Project.pdf\n",
            "\tREADME.md\n",
            "\tcleaned_Medical Translation.csv\n",
            "Please move or remove them before you merge.\n",
            "Aborting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Push Your Changes to GitHub\n",
        "# After successfully pulling the latest changes and resolving any conflicts (if needed), you can now push your local changes:\n",
        "\n",
        "!git push https://{os.environ['GITHUB_TOKEN']}@github.com/Merha23/AI_Tigrinya_Translation.git main"
      ],
      "metadata": {
        "id": "3xFNoqGS2L9e",
        "outputId": "e47801cd-54fb-49a4-e7ef-fd1a24c14a69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: src refspec main does not match any\n",
            "\u001b[31merror: failed to push some refs to 'https://github.com/Merha23/AI_Tigrinya_Translation.git'\n",
            "\u001b[m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Push Updates from Colab to GitHub\n",
        "# After editing files, commit and push:\n",
        "\n",
        "!git config --global user.email \"merhagebrelibanos29@gmail.com\"\n",
        "!git config --global user.name \"Merha Gebrelibanos\"\n",
        "!git add .\n",
        "!git commit -m \"Updated final project source code program\"\n",
        "!git push https://{os.environ['GITHUB_TOKEN']}@github.com/Merha23/AI_Tigrinya_Translation.git main"
      ],
      "metadata": {
        "id": "FNW6F7_wQQPO",
        "outputId": "62d3c83f-7bd3-4b1f-af94-3dd704ff52f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[master (root-commit) 4c22ab6] Updated final project source code program\n",
            " 11 files changed, 4469 insertions(+)\n",
            " create mode 100644 .gitignore\n",
            " create mode 100644 Cheat_Sheet_Python_Code.ipynb\n",
            " create mode 100644 Cleaned dataset/Train and test.md\n",
            " create mode 100644 Cleaned dataset/cleaned_Medical Translation (5).csv\n",
            " create mode 100644 Final_Project_Source_Code_Program.ipynb\n",
            " create mode 100644 LICENSE\n",
            " create mode 100644 Medical Translation.csv\n",
            " create mode 100644 Practices.ipynb\n",
            " create mode 100644 Proposal for Final Project.pdf\n",
            " create mode 100644 README.md\n",
            " create mode 100644 cleaned_Medical Translation.csv\n",
            "error: src refspec main does not match any\n",
            "\u001b[31merror: failed to push some refs to 'https://github.com/Merha23/AI_Tigrinya_Translation.git'\n",
            "\u001b[m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)    **Load the CSV File into a Pandas DataFrame**"
      ],
      "metadata": {
        "id": "Twi6eepd6KF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"Medical Translation.csv\", encoding='utf-8')\n",
        "df.head()  # Display first few rows"
      ],
      "metadata": {
        "id": "U5liXc1FIA0g",
        "outputId": "6efc52e0-c324-44dd-817b-79c8c63fc599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                            english  \\\n",
              "0   1                            Information for parents   \n",
              "1   2  If you choose not to vaccinate your child, und...   \n",
              "2   3  Telling healthcare professionals your child’s ...   \n",
              "3   4  If you choose to delay some vaccines or refuse...   \n",
              "4   5  Please follow these steps to protect your chil...   \n",
              "\n",
              "                                            tigrinya   domain  \n",
              "0                                     ንወለዲ ዝኸውን ሓበሬታ  medical  \n",
              "1         ውላድኩም ከይተኽትብዎ እንተመሪጽኩም፣ ሓደጋታትን ሓላፍነትን ተረድኡ  medical  \n",
              "2  ንሰብ ሞያ ክንክን ጥዕና ኩነታት ክታበት ውላድኩም ምሕባር ብኽልተ ምኽንያ...  medical  \n",
              "3  ንገሊኡ ክታበታት ክተደናጉዩ ወይ ንገሊኡ ክታበታት ምሉእ ብምሉእ ክትነጽጉ...  medical  \n",
              "4  ንውላድኩም፣ ንስድራቤትኩምን ንኻልኦትን ንምክልኻል በጃኹም ነዞም ስጉምትታ...  medical  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bd01ac9a-2e02-4fbd-b383-6d7d4db82c63\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>english</th>\n",
              "      <th>tigrinya</th>\n",
              "      <th>domain</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Information for parents</td>\n",
              "      <td>ንወለዲ ዝኸውን ሓበሬታ</td>\n",
              "      <td>medical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>If you choose not to vaccinate your child, und...</td>\n",
              "      <td>ውላድኩም ከይተኽትብዎ እንተመሪጽኩም፣ ሓደጋታትን ሓላፍነትን ተረድኡ</td>\n",
              "      <td>medical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Telling healthcare professionals your child’s ...</td>\n",
              "      <td>ንሰብ ሞያ ክንክን ጥዕና ኩነታት ክታበት ውላድኩም ምሕባር ብኽልተ ምኽንያ...</td>\n",
              "      <td>medical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you choose to delay some vaccines or refuse...</td>\n",
              "      <td>ንገሊኡ ክታበታት ክተደናጉዩ ወይ ንገሊኡ ክታበታት ምሉእ ብምሉእ ክትነጽጉ...</td>\n",
              "      <td>medical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Please follow these steps to protect your chil...</td>\n",
              "      <td>ንውላድኩም፣ ንስድራቤትኩምን ንኻልኦትን ንምክልኻል በጃኹም ነዞም ስጉምትታ...</td>\n",
              "      <td>medical</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bd01ac9a-2e02-4fbd-b383-6d7d4db82c63')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bd01ac9a-2e02-4fbd-b383-6d7d4db82c63 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bd01ac9a-2e02-4fbd-b383-6d7d4db82c63');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-db657cc6-9e21-492b-809e-b31e87dbf3f9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-db657cc6-9e21-492b-809e-b31e87dbf3f9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-db657cc6-9e21-492b-809e-b31e87dbf3f9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 750,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 216,\n        \"min\": 1,\n        \"max\": 750,\n        \"num_unique_values\": 750,\n        \"samples\": [\n          507,\n          358,\n          134\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"english\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 700,\n        \"samples\": [\n          \"REFERRALS:\",\n          \"<1>Only participating candidates running for City Council, City Attorney, or Mayor in Seattle can accept your vouchers to support their campaigns.\",\n          \"For more information, please visit ORR\\u2019s website at {/t2}{a1}https://www.acf.hhs.gov//orr/programs/refugees{/a1}\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tigrinya\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 705,\n        \"samples\": [\n          \"\\u1293\\u1265 617-694-5949 \\u12f0\\u12cd\\u1209\",\n          \"\\u1290\\u12da \\u12ad\\u120a\\u1292\\u12ad\\u1295/\\u12c8\\u12ed \\u12f5\\u121b \\u121d\\u1235\\u12a1 \\u12dd\\u1270\\u12a3\\u1233\\u1230\\u1229 \\u12a3\\u12ab\\u120b\\u1275\\u1295 \\u12a5\\u1348\\u1255\\u12f0\\u120e\\u121d\\u1364 \\u1213\\u12ab\\u12ed\\u121d\\u1295 \\u12c8\\u1203\\u1265\\u1272 \\u120b\\u12d5\\u1208\\u12cb\\u12ed \\u120d\\u121d\\u12f2\\u1363 (\\u1265\\u1320\\u1255\\u120b\\u120b\\u1363 \\u201c\\u12c8\\u1203\\u1265\\u1272 \\u12a3\\u1308\\u120d\\u130d\\u120e\\u1275\\u2019)\\u1364 \\u12a8\\u121d\\u12a1\\u2019\\u12cd\\u1295 \\u12c8\\u12a8\\u120d\\u1276\\u121d\\u1295 \\u1230\\u122b\\u1215\\u1270\\u129b\\u1273\\u1276\\u121d\\u1295 \\u1215\\u12ad\\u121d\\u1293\\u12ca\\u1363 \\u1325\\u12d5\\u1293 \\u12a3\\u12a5\\u121d\\u122e\\u1295 \\u1218\\u1325\\u1263\\u1215\\u1273\\u12ca \\u12ad\\u1295\\u12ad\\u1295\\u1295 \\u1295\\u12bd\\u1205\\u1261\\u1292\\u1362\",\n          \"3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"domain\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"legal\",\n          \"medical\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "jJB419aCfEx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)     **Data Cleaning & Handling Missing Values**\n",
        "\n",
        "     Before tokenization, we need to remove any unnecessary data such as missing values, duplicates, or improperly formatted sentences."
      ],
      "metadata": {
        "id": "fuPAJYJ460-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert both 'english' and 'tigrinya' columns to strings (to handle float values as well)\n",
        "df['english'] = df['english'].astype(str)\n",
        "df['tigrinya'] = df['tigrinya'].astype(str)"
      ],
      "metadata": {
        "id": "MuoLs8dLcvo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values (NaN) in the columns with an empty string\n",
        "df['english'] = df['english'].fillna(\"\")\n",
        "df['tigrinya'] = df['tigrinya'].fillna(\"\")"
      ],
      "metadata": {
        "id": "vWVZ3zk2eNAx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for non-string values in 'english' and 'tigrinya'\n",
        "print(df['english'].apply(type).value_counts())\n",
        "print(df['tigrinya'].apply(type).value_counts())"
      ],
      "metadata": {
        "id": "kXyXMrbFdvb7",
        "outputId": "ec91fedc-03b1-4876-e4dd-a6ae1f5378e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "english\n",
            "<class 'str'>    750\n",
            "Name: count, dtype: int64\n",
            "tigrinya\n",
            "<class 'str'>    750\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove empty or missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Strip unwanted whitespace\n",
        "df['english'] = df['english'].str.strip()\n",
        "df['tigrinya'] = df['tigrinya'].str.strip()\n",
        "\n",
        "print(f\"Dataset size after cleaning: {df.shape}\")"
      ],
      "metadata": {
        "id": "7Jj7fNc5JQwP",
        "outputId": "29e20eed-f0c8-48da-e68d-1e087d285ec7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size after cleaning: (750, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) **Sentence Tokenization**:\n",
        "\n",
        "  Since we are training a sequence-to-sequence model, we must tokenize each sentence."
      ],
      "metadata": {
        "id": "PIIFIxS8KW0l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tJKicMlbgwNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset into train (80%), validation (10%), and test (10%)\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df['english'].tolist(), df['tigrinya'].tolist(), test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts, temp_labels, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_data = Dataset.from_dict({\"english\": train_texts, \"tigrinya\": train_labels})\n",
        "val_data = Dataset.from_dict({\"english\": val_texts, \"tigrinya\": val_labels})\n",
        "test_data = Dataset.from_dict({\"english\": test_texts, \"tigrinya\": test_labels})\n",
        "\n",
        "# Prepare the DatasetDict for tokenization\n",
        "datasets = DatasetDict({\n",
        "    \"train\": train_data,\n",
        "    \"validation\": val_data,\n",
        "    \"test\": test_data\n",
        "})\n"
      ],
      "metadata": {
        "id": "nWpuFZELsqNM",
        "outputId": "874cc367-0ae3-4f44-f243-c56ff5695350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-2b32a192f398>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Split dataset into train (80%), validation (10%), and test (10%)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"  # Replace with your model name if different\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define a tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['english'], examples['tigrinya'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Apply the tokenization function to the datasets\n",
        "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n"
      ],
      "metadata": {
        "id": "6uwyW9x7s5b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./nllb_trained',  # Output directory for saving model checkpoints\n",
        "    eval_strategy=\"epoch\",  # Evaluate after each epoch (use eval_strategy instead of evaluation_strategy)\n",
        "    learning_rate=2e-5,  # Learning rate\n",
        "    per_device_train_batch_size=8,  # Batch size for training\n",
        "    per_device_eval_batch_size=8,  # Batch size for evaluation\n",
        "    num_train_epochs=3,  # Number of epochs\n",
        "    weight_decay=0.01,  # Weight decay for regularization\n",
        "    save_steps=500,  # Save model every 500 steps\n",
        "    logging_dir='./logs',  # Directory for logs\n",
        "    logging_steps=100,  # Log every 100 steps\n",
        "    report_to=\"none\"  # Disable logging to WandB\n",
        ")\n"
      ],
      "metadata": {
        "id": "bGFEvY6tvJNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show wandb\n"
      ],
      "metadata": {
        "id": "GvNtRNIdzx7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $WANDB_API_KEY\n"
      ],
      "metadata": {
        "id": "aj-b9FGA0FVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall wandb\n"
      ],
      "metadata": {
        "id": "xAXOfIfh0Z0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize the data\n",
        "def tokenize_function(examples):\n",
        "    model_inputs = tokenizer(examples[\"english\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"tigrinya\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenizing the datasets\n",
        "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
        "\n",
        "# Prepare training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./nllb_trained',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    report_to=\"none\"  # Disable W&B logging (optional)\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "d3QPksrD4Z4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import TrainingArguments, Trainer, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Disable WandB logging globally by setting this environment variable\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Load the pre-trained model\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"  # Replace with your model name if different\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load your dataset (replace this with your actual dataset)\n",
        "# tokenized_datasets = load_dataset('your_dataset')\n",
        "\n",
        "# Prepare your training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./nllb_trained',  # Output directory for saving model checkpoints\n",
        "    eval_strategy=\"epoch\",  # Use eval_strategy instead of the deprecated evaluation_strategy\n",
        "    learning_rate=2e-5,  # Learning rate\n",
        "    per_device_train_batch_size=8,  # Train batch size\n",
        "    per_device_eval_batch_size=8,   # Evaluation batch size\n",
        "    num_train_epochs=3,  # Number of training epochs\n",
        "    save_steps=10_000,  # Save model every 10k steps\n",
        "    save_total_limit=2,  # Limit the total saved models\n",
        "    logging_dir='./logs',  # Directory for logs\n",
        "    logging_strategy=\"steps\",  # Log every set number of steps\n",
        "    logging_steps=500,  # Log every 500 steps\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],  # Ensure the dataset is loaded correctly\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],  # Use validation data\n",
        "    tokenizer=tokenizer,  # Tokenizer for data processing\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "iKYVeTG835zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the pre-trained model\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"  # Replace with your model name if different\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load dataset (replace this with your dataset)\n",
        "# tokenized_datasets = load_dataset('your_dataset')\n",
        "\n",
        "# Prepare your training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./nllb_trained',  # Output directory for saving model checkpoints\n",
        "    eval_strategy=\"epoch\",  # Use eval_strategy instead of the deprecated evaluation_strategy\n",
        "    learning_rate=2e-5,  # Learning rate\n",
        "    per_device_train_batch_size=8,  # Train batch size\n",
        "    per_device_eval_batch_size=8,   # Evaluation batch size\n",
        "    num_train_epochs=3,  # Number of training epochs\n",
        "    save_steps=10_000,  # Save model every 10k steps\n",
        "    save_total_limit=2,  # Limit the total saved models\n",
        "    report_to=None,  # Explicitly disable WandB logging\n",
        "    logging_dir='./logs',  # Directory for logs\n",
        "    logging_strategy=\"steps\",  # Log every set number of steps\n",
        "    logging_steps=500,  # Log every 500 steps\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],  # Ensure the dataset is loaded correctly\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],  # Use validation data\n",
        "    tokenizer=tokenizer,  # Tokenizer for data processing\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "yDUy0L8Gy6F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    processing_class=tokenizer,  # Use processing_class instead of tokenizer\n",
        ")\n"
      ],
      "metadata": {
        "id": "j_GkiwssxHpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Define Training Arguments (without 'predict_with_generate')\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./nllb_trained',  # Output directory for saving model checkpoints\n",
        "    eval_strategy=\"epoch\",  # Evaluate after each epoch (use eval_strategy instead of evaluation_strategy)\n",
        "    learning_rate=2e-5,  # Learning rate\n",
        "    per_device_train_batch_size=8,  # Batch size for training\n",
        "    per_device_eval_batch_size=8,  # Batch size for evaluation\n",
        "    num_train_epochs=3,  # Number of epochs\n",
        "    weight_decay=0.01,  # Weight decay for regularization\n",
        "    save_steps=500,  # Save model every 500 steps\n",
        "    logging_dir='./logs',  # Directory for logs\n",
        "    logging_steps=100  # Log every 100 steps\n",
        ")\n",
        "\n",
        "# Load the pre-trained model\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"  # Replace with your model name if different\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Initialize the Trainer (no need to pass 'predict_with_generate')\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer  # No longer need to explicitly set `predict_with_generate`\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "19YJbRy8t1Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Define Training Arguments without `predict_with_generate`\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./nllb_trained',  # Output directory for saving model checkpoints\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
        "    learning_rate=2e-5,  # Learning rate\n",
        "    per_device_train_batch_size=8,  # Batch size for training\n",
        "    per_device_eval_batch_size=8,  # Batch size for evaluation\n",
        "    num_train_epochs=3,  # Number of epochs\n",
        "    weight_decay=0.01,  # Weight decay for regularization\n",
        "    save_steps=500,  # Save model every 500 steps\n",
        "    logging_dir='./logs',  # Directory for logs\n",
        "    logging_steps=100  # Log every 100 steps\n",
        ")\n",
        "\n",
        "# Load the pre-trained model\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"  # Replace with your model name if different\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Initialize the Trainer with `predict_with_generate` set in the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    predict_with_generate=True  # Set this inside the Trainer\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "vOkCtzrctQ50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./nllb_trained',  # Output directory for saving model checkpoints\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
        "    learning_rate=2e-5,  # Learning rate\n",
        "    per_device_train_batch_size=8,  # Batch size for training\n",
        "    per_device_eval_batch_size=8,  # Batch size for evaluation\n",
        "    num_train_epochs=3,  # Number of epochs\n",
        "    weight_decay=0.01,  # Weight decay for regularization\n",
        "    save_steps=500,  # Save model every 500 steps\n",
        "    logging_dir='./logs',  # Directory for logs\n",
        "    logging_steps=100,  # Log every 100 steps\n",
        "    predict_with_generate=True  # Use generate method for evaluation\n",
        ")\n"
      ],
      "metadata": {
        "id": "1--Lf6L8tEg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"Medical Translation.csv\")  # Adjust the path if necessary\n",
        "\n",
        "# Split dataset into train (80%), validation (10%), and test (10%)\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df[\"english\"].tolist(), df[\"tigrinya\"].tolist(), test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts, temp_labels, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_data = Dataset.from_dict({\"english\": train_texts, \"tigrinya\": train_labels})\n",
        "val_data = Dataset.from_dict({\"english\": val_texts, \"tigrinya\": val_labels})\n",
        "test_data = Dataset.from_dict({\"english\": test_texts, \"tigrinya\": test_labels})\n",
        "\n",
        "# Prepare the DatasetDict for tokenization\n",
        "datasets = DatasetDict({\n",
        "    \"train\": train_data,\n",
        "    \"validation\": val_data,\n",
        "    \"test\": test_data\n",
        "})\n"
      ],
      "metadata": {
        "id": "Zqes3fVIpdPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Ensure column names match exactly\n",
        "df['tokenized_english'] = df['english'].apply(lambda x: str(x).split('. '))\n",
        "df['tokenized_tigrinya'] = df['tigrinya'].apply(lambda x: str(x).split('። '))\n",
        "df[['tokenized_english', 'tokenized_tigrinya']].head()"
      ],
      "metadata": {
        "id": "AS7jeQQzOSqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "                **Check Sentence Length Distribution**\n",
        "\n",
        "We need to analyze the length of sentences to ensure they fit within the model's input constraints."
      ],
      "metadata": {
        "id": "DZVF_VRO2MnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check sentence length distribution\n",
        "df['eng_length'] = df['english'].apply(lambda x: len(str(x).split()))\n",
        "df['tir_length'] = df['tigrinya'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "df[['eng_length', 'tir_length']].describe()"
      ],
      "metadata": {
        "id": "PPbHNVbt2euZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "            **Filtering Extremely Long or Short Sentences**\n",
        "\n",
        "Very short or long sentences might reduce model performance. We filter out sentences that are too short (<3 words) or too long (>128 words)."
      ],
      "metadata": {
        "id": "QaiSvFjH4_MD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out sentences that are too short or too long\n",
        "df = df[(df['eng_length'] >= 3) & (df['eng_length'] <= 128)]\n",
        "df = df[(df['tir_length'] >= 3) &  (df['tir_length'] <= 128)]\n",
        "\n",
        "print(f\"Dataset size after filtering: {df.shape}\")"
      ],
      "metadata": {
        "id": "2q0HZ3UC-igl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)  # Verify column names"
      ],
      "metadata": {
        "id": "4myKXvBFunuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "               **Save the Processed Dataset**\n",
        "\n",
        "After preprocessing, we save the cleaned and tokenized dataset for use in model training."
      ],
      "metadata": {
        "id": "i1hH_5Hq_BJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"cleaned_Medical Translation.csv\", index=False, encoding='utf-8')\n",
        "print(\"Dataset saved successfully!\")"
      ],
      "metadata": {
        "id": "OexR0oG2_PTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())  # Shows current directory\n",
        "print(os.listdir())  # Lists all files in the directory\n"
      ],
      "metadata": {
        "id": "3qZXzfQ4C_YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_path = \"/content/AI_Tigrinya_Translation/cleaned_Medical Translation.csv\"\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    print(\"File exists:\", file_path)\n",
        "else:\n",
        "    print(\"File does NOT exist!\")\n"
      ],
      "metadata": {
        "id": "RDS-zycpU5H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get the current working directory\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "# List all files in the directory\n",
        "files = os.listdir(current_dir)\n",
        "\n",
        "# Print full paths\n",
        "for file in files:\n",
        "    full_path = os.path.join(current_dir, file)\n",
        "    print(full_path)"
      ],
      "metadata": {
        "id": "LVuMaCSkOHXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tokenized_tigrinya'] = df['tokenized_tigrinya'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)"
      ],
      "metadata": {
        "id": "O1m_f9Osp3I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"cleaned_dataset.csv\", index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "J77SJy_Qq1h2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['tokenized_english', 'tokenized_tigrinya']].head(10)"
      ],
      "metadata": {
        "id": "dyLsEq_zqPF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To download the cleaned dataset\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"cleaned_Medical Translation.csv\")"
      ],
      "metadata": {
        "id": "5_tKRcXiDbe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "      **Fine-Tuning NLLB-200 Model **\n",
        "\n",
        "      Now that we have a cleaned and tokenized dataset,\n",
        "      we can fine-tune the NLLB-200 model to improve translation\n",
        "      performance for English ⇆ Tigrinya in medical and legal contexts.\n",
        "\n",
        "      **Install and Load Hugging Face Transformers**\n",
        "\n",
        "Before using the model, install and import the necessary libraries:"
      ],
      "metadata": {
        "id": "knkWVZIwtLX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "dqz00p_3yz4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Pretrained NLLB-200 Model**\n",
        "\n",
        "We will use Facebook's NLLB-200 model, specifically the distilled 600M version, which is optimized for translation tasks."
      ],
      "metadata": {
        "id": "uabWQMzqz5zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "Las73mZvOK6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"cleaned_Medical Translation.csv\")\n",
        "print(df.head())  # Show first few rows\n",
        "print(df.columns)  # Display column names\n"
      ],
      "metadata": {
        "id": "1Qa_C7KwTHZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = df.columns.str.strip()  # Remove leading/trailing spaces\n",
        "df.rename(columns={\"english\": \"source\", \"tigrinya\": \"target\"}, inplace=True)\n"
      ],
      "metadata": {
        "id": "OKqL8F7tTQey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df[\"source\"].tolist(), df[\"target\"].tolist(), test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "5WxcqruoTj91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "UubNeIojSLLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"cleaned_Medical Translation.csv\")\n",
        "\n",
        "# Debug: Print column names\n",
        "print(\"Dataset Columns:\", df.columns)\n",
        "\n",
        "# Rename columns to match expected format\n",
        "df.rename(columns={\"english\": \"source\", \"tigrinya\": \"target\"}, inplace=True)\n",
        "\n",
        "# Split dataset into train (80%), validation (10%), and test (10%)\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df[\"source\"], df[\"target\"], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts, temp_labels, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_dict({\"source\": train_texts.tolist(), \"target\": train_labels.tolist()})\n",
        "val_dataset = Dataset.from_dict({\"source\": val_texts.tolist(), \"target\": val_labels.tolist()})\n",
        "test_dataset = Dataset.from_dict({\"source\": test_texts.tolist(), \"target\": test_labels.tolist()})\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": val_dataset,\n",
        "    \"test\": test_dataset,\n",
        "})\n",
        "\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "YF4g90l1Ugrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"  # Choose appropriate NLLB-200 variant\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer(examples[\"source\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(examples[\"target\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "print(\"Tokenization complete!\")\n"
      ],
      "metadata": {
        "id": "SYhjNgcAVSf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade transformers\n"
      ],
      "metadata": {
        "id": "GNBDx75uWLlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Define the model name (Use the actual model name you're working with)\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"  # Or another variant\n",
        "\n",
        "# Load Pretrained Model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "cDIxJsYQXrzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the dataset\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer(examples[\"english\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(examples[\"tigrinya\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_data = Dataset.from_dict({\"english\": train_texts, \"tigrinya\": train_labels})\n",
        "val_data = Dataset.from_dict({\"english\": val_texts, \"tigrinya\": val_labels})\n",
        "test_data = Dataset.from_dict({\"english\": test_texts, \"tigrinya\": test_labels})\n",
        "\n",
        "# Apply the tokenization function\n",
        "tokenized_datasets = DatasetDict({\n",
        "    \"train\": train_data.map(preprocess_function, batched=True),\n",
        "    \"validation\": val_data.map(preprocess_function, batched=True),\n",
        "    \"test\": test_data.map(preprocess_function, batched=True),\n",
        "})\n",
        "\n",
        "print(\"Tokenization complete!\")"
      ],
      "metadata": {
        "id": "_FKaJWHCZ8W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./nllb_trained\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,  # Use FP16 if on GPU\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=500,\n",
        ")\n"
      ],
      "metadata": {
        "id": "mHj-_TZinnyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Define the model name\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"  # Or the path to your fine-tuned model\n",
        "\n",
        "# Load Pretrained Model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, Trainer\n",
        "\n",
        "# Load Pretrained Model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "kuc004Bknyhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare Data for Fine-Tuning**\n",
        "\n",
        "We need to format our cleaned dataset to be compatible with the model."
      ],
      "metadata": {
        "id": "2GI7Zy0I04vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(examples[\"english\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    targets = tokenizer(examples[\"tigrinya\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "# Load cleaned dataset\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"cleaned_Medical Translation.csv\")\n",
        "\n",
        "# Convert to Hugging Face dataset format\n",
        "dataset = Dataset.from_pandas(df)\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "ysk8D0XE0_8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Training Arguments**\n",
        "\n",
        "We set up configurations for fine-tuning, such as batch size, learning rate, and evaluation strategy."
      ],
      "metadata": {
        "id": "uFJowBCK1p0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine-tune the Model**\n",
        "\n",
        "Now you can fine-tune the NLLB-200 model using the training arguments you defined. Make sure that you have your dataset loaded and preprocessed correctly."
      ],
      "metadata": {
        "id": "QCE97-rvKGSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "id": "v8q4618ZM52Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Load the pre-trained NLLB-200 model and tokenizer\n",
        "model_name = \"facebook/nllb-200-3.3B\"  # Replace with the correct NLLB model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "an0V_zjDNj8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "KXKMxTjoOQ7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls -la"
      ],
      "metadata": {
        "id": "I-le8vqvR6XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find . -name \".gitignore\""
      ],
      "metadata": {
        "id": "3igQmi3VTEuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find . -name \"cleaned_Medical Translation.CSV\""
      ],
      "metadata": {
        "id": "lZ5Drd5pSH2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls | grep \"cleaned_Medical Translation.csv\"\n"
      ],
      "metadata": {
        "id": "gzwaWq5_fV-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git ls-files | grep \"cleaned_Medical Translation.CSV\""
      ],
      "metadata": {
        "id": "bZATARVrTxWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git ls-files | grep \".env\""
      ],
      "metadata": {
        "id": "carbjbUoUAhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Pretrained NLLB-200 Model"
      ],
      "metadata": {
        "id": "Bu4hfh8AvKtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Data for Fine-Tuning"
      ],
      "metadata": {
        "id": "AJofj-RWv2p4"
      }
    }
  ]
}