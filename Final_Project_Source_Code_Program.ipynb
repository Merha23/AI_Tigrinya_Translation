{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Merha23/AI_Tigrinya_Translation/blob/main/Final_Project_Source_Code_Program.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "      **Integration GitHub with Google Colab**"
      ],
      "metadata": {
        "id": "I2mjbZgs5RLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the GitHub Repository in Google Colab\n",
        "# Authenticate Google Colab with GitHub\n",
        "# Clone with Authentication\n",
        "# Generate a GitHub Personal Access Token (PAT)\n",
        "# How to Use Your GitHub Token in Google Colab Securely\n",
        "# cloning your repository using the token:\n",
        "# Use the stored token to clone your GitHub repository in Google Colab:\n",
        "# Clone the Repository Securely in Colab\n",
        "# Instead of using your username/password, use the token as follows:\n",
        "\n",
        "# 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   Personal Access Token (PAT)\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "token = getpass('Enter your GitHub Personal Access Token: ')\n",
        "os.environ[\"GITHUB_TOKEN\"] = token\n",
        "\n",
        "repo_url = f\"https://{token}@github.com/Merha23/AI_Tigrinya_Translation.git\"\n",
        "!git clone {repo_url}\n",
        "%cd AI_Tigrinya_Translation\n"
      ],
      "metadata": {
        "id": "MN7ZR_4k6veK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf .git\n",
        "!git init\n"
      ],
      "metadata": {
        "id": "HLmPia8hkGQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/AI_Tigrinya_Translation\n"
      ],
      "metadata": {
        "id": "jgj4YCfYkYXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this python code and if cloning is successful, it should display the repository contents.\n",
        "\n",
        "!ls -la\n"
      ],
      "metadata": {
        "id": "LehnBNoMKIQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive in Colab\n",
        "# Run this command in Colab to access your Drive:\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "M9JMtl6s95zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull the Latest Changes from GitHub\n",
        "# Before pushing your changes, you need to sync your local copy of the repository with the remote one.\n",
        "# Pull the latest changes:\n",
        "# Run the following command to fetch and merge the latest changes from GitHub into your local branch:\n",
        "\n",
        "!git pull https://{os.environ['GITHUB_TOKEN']}@github.com/Merha23/AI_Tigrinya_Translation.git main"
      ],
      "metadata": {
        "id": "z_N_oHc90qRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push Your Changes to GitHub\n",
        "# After successfully pulling the latest changes and resolving any conflicts (if needed), you can now push your local changes:\n",
        "\n",
        "!git push https://{os.environ['GITHUB_TOKEN']}@github.com/Merha23/AI_Tigrinya_Translation.git main"
      ],
      "metadata": {
        "id": "3xFNoqGS2L9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To reset to the remote state (undo local changes):\n",
        "\n",
        "!git reset --hard origin/main\n"
      ],
      "metadata": {
        "id": "364tn3hO2r17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push Updates from Colab to GitHub\n",
        "# After editing files, commit and push:\n",
        "\n",
        "!git config --global user.email \"merhagebrelibanos29@gmail.com\"\n",
        "!git config --global user.name \"Merha Gebrelibanos\"\n",
        "!git add .\n",
        "!git commit -m \"Updated final project source code program\"\n",
        "!git push https://{os.environ['GITHUB_TOKEN']}@github.com/Merha23/AI_Tigrinya_Translation.git main"
      ],
      "metadata": {
        "id": "FNW6F7_wQQPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)    **Load the CSV File into a Pandas DataFrame**"
      ],
      "metadata": {
        "id": "Twi6eepd6KF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"Medical Translation.csv\", encoding='utf-8')\n",
        "df.head()  # Display first few rows"
      ],
      "metadata": {
        "id": "U5liXc1FIA0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "jJB419aCfEx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)     **Data Cleaning & Handling Missing Values**\n",
        "\n",
        "     Before tokenization, we need to remove any unnecessary data such as missing values, duplicates, or improperly formatted sentences."
      ],
      "metadata": {
        "id": "fuPAJYJ460-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert both 'english' and 'tigrinya' columns to strings (to handle float values as well)\n",
        "df['english'] = df['english'].astype(str)\n",
        "df['tigrinya'] = df['tigrinya'].astype(str)"
      ],
      "metadata": {
        "id": "MuoLs8dLcvo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values (NaN) in the columns with an empty string\n",
        "df['english'] = df['english'].fillna(\"\")\n",
        "df['tigrinya'] = df['tigrinya'].fillna(\"\")"
      ],
      "metadata": {
        "id": "vWVZ3zk2eNAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for non-string values in 'english' and 'tigrinya'\n",
        "print(df['english'].apply(type).value_counts())\n",
        "print(df['tigrinya'].apply(type).value_counts())"
      ],
      "metadata": {
        "id": "kXyXMrbFdvb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove empty or missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Strip unwanted whitespace\n",
        "df['english'] = df['english'].str.strip()\n",
        "df['tigrinya'] = df['tigrinya'].str.strip()\n",
        "\n",
        "print(f\"Dataset size after cleaning: {df.shape}\")"
      ],
      "metadata": {
        "id": "7Jj7fNc5JQwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) **Sentence Tokenization**:\n",
        "\n",
        "  Since we are training a sequence-to-sequence model, we must tokenize each sentence."
      ],
      "metadata": {
        "id": "PIIFIxS8KW0l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tJKicMlbgwNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset into train (80%), validation (10%), and test (10%)\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df['english'].tolist(), df['tigrinya'].tolist(), test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts, temp_labels, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_data = Dataset.from_dict({\"english\": train_texts, \"tigrinya\": train_labels})\n",
        "val_data = Dataset.from_dict({\"english\": val_texts, \"tigrinya\": val_labels})\n",
        "test_data = Dataset.from_dict({\"english\": test_texts, \"tigrinya\": test_labels})\n",
        "\n",
        "# Prepare the DatasetDict for tokenization\n",
        "datasets = DatasetDict({\n",
        "    \"train\": train_data,\n",
        "    \"validation\": val_data,\n",
        "    \"test\": test_data\n",
        "})\n"
      ],
      "metadata": {
        "id": "nWpuFZELsqNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"  # Replace with your model name if different\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define a tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['english'], examples['tigrinya'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Apply the tokenization function to the datasets\n",
        "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n"
      ],
      "metadata": {
        "id": "6uwyW9x7s5b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./nllb_trained',  # Output directory for saving model checkpoints\n",
        "    eval_strategy=\"epoch\",  # Evaluate after each epoch (use eval_strategy instead of evaluation_strategy)\n",
        "    learning_rate=2e-5,  # Learning rate\n",
        "    per_device_train_batch_size=8,  # Batch size for training\n",
        "    per_device_eval_batch_size=8,  # Batch size for evaluation\n",
        "    num_train_epochs=3,  # Number of epochs\n",
        "    weight_decay=0.01,  # Weight decay for regularization\n",
        "    save_steps=500,  # Save model every 500 steps\n",
        "    logging_dir='./logs',  # Directory for logs\n",
        "    logging_steps=100,  # Log every 100 steps\n",
        "    report_to=\"none\"  # Disable logging to WandB\n",
        ")\n"
      ],
      "metadata": {
        "id": "bGFEvY6tvJNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show wandb\n"
      ],
      "metadata": {
        "id": "GvNtRNIdzx7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $WANDB_API_KEY\n"
      ],
      "metadata": {
        "id": "aj-b9FGA0FVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall wandb\n"
      ],
      "metadata": {
        "id": "xAXOfIfh0Z0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize the data\n",
        "def tokenize_function(examples):\n",
        "    model_inputs = tokenizer(examples[\"english\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"tigrinya\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenizing the datasets\n",
        "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
        "\n",
        "# Prepare training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./nllb_trained',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    report_to=\"none\"  # Disable W&B logging (optional)\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "d3QPksrD4Z4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import TrainingArguments, Trainer, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Disable WandB logging globally by setting this environment variable\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Load the pre-trained model\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"  # Replace with your model name if different\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load your dataset (replace this with your actual dataset)\n",
        "# tokenized_datasets = load_dataset('your_dataset')\n",
        "\n",
        "# Prepare your training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./nllb_trained',  # Output directory for saving model checkpoints\n",
        "    eval_strategy=\"epoch\",  # Use eval_strategy instead of the deprecated evaluation_strategy\n",
        "    learning_rate=2e-5,  # Learning rate\n",
        "    per_device_train_batch_size=8,  # Train batch size\n",
        "    per_device_eval_batch_size=8,   # Evaluation batch size\n",
        "    num_train_epochs=3,  # Number of training epochs\n",
        "    save_steps=10_000,  # Save model every 10k steps\n",
        "    save_total_limit=2,  # Limit the total saved models\n",
        "    logging_dir='./logs',  # Directory for logs\n",
        "    logging_strategy=\"steps\",  # Log every set number of steps\n",
        "    logging_steps=500,  # Log every 500 steps\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],  # Ensure the dataset is loaded correctly\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],  # Use validation data\n",
        "    tokenizer=tokenizer,  # Tokenizer for data processing\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "iKYVeTG835zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the pre-trained model\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"  # Replace with your model name if different\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load dataset (replace this with your dataset)\n",
        "# tokenized_datasets = load_dataset('your_dataset')\n",
        "\n",
        "# Prepare your training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./nllb_trained',  # Output directory for saving model checkpoints\n",
        "    eval_strategy=\"epoch\",  # Use eval_strategy instead of the deprecated evaluation_strategy\n",
        "    learning_rate=2e-5,  # Learning rate\n",
        "    per_device_train_batch_size=8,  # Train batch size\n",
        "    per_device_eval_batch_size=8,   # Evaluation batch size\n",
        "    num_train_epochs=3,  # Number of training epochs\n",
        "    save_steps=10_000,  # Save model every 10k steps\n",
        "    save_total_limit=2,  # Limit the total saved models\n",
        "    report_to=None,  # Explicitly disable WandB logging\n",
        "    logging_dir='./logs',  # Directory for logs\n",
        "    logging_strategy=\"steps\",  # Log every set number of steps\n",
        "    logging_steps=500,  # Log every 500 steps\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],  # Ensure the dataset is loaded correctly\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],  # Use validation data\n",
        "    tokenizer=tokenizer,  # Tokenizer for data processing\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "yDUy0L8Gy6F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    processing_class=tokenizer,  # Use processing_class instead of tokenizer\n",
        ")\n"
      ],
      "metadata": {
        "id": "j_GkiwssxHpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Define Training Arguments (without 'predict_with_generate')\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./nllb_trained',  # Output directory for saving model checkpoints\n",
        "    eval_strategy=\"epoch\",  # Evaluate after each epoch (use eval_strategy instead of evaluation_strategy)\n",
        "    learning_rate=2e-5,  # Learning rate\n",
        "    per_device_train_batch_size=8,  # Batch size for training\n",
        "    per_device_eval_batch_size=8,  # Batch size for evaluation\n",
        "    num_train_epochs=3,  # Number of epochs\n",
        "    weight_decay=0.01,  # Weight decay for regularization\n",
        "    save_steps=500,  # Save model every 500 steps\n",
        "    logging_dir='./logs',  # Directory for logs\n",
        "    logging_steps=100  # Log every 100 steps\n",
        ")\n",
        "\n",
        "# Load the pre-trained model\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"  # Replace with your model name if different\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Initialize the Trainer (no need to pass 'predict_with_generate')\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer  # No longer need to explicitly set `predict_with_generate`\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "19YJbRy8t1Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Define Training Arguments without `predict_with_generate`\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./nllb_trained',  # Output directory for saving model checkpoints\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
        "    learning_rate=2e-5,  # Learning rate\n",
        "    per_device_train_batch_size=8,  # Batch size for training\n",
        "    per_device_eval_batch_size=8,  # Batch size for evaluation\n",
        "    num_train_epochs=3,  # Number of epochs\n",
        "    weight_decay=0.01,  # Weight decay for regularization\n",
        "    save_steps=500,  # Save model every 500 steps\n",
        "    logging_dir='./logs',  # Directory for logs\n",
        "    logging_steps=100  # Log every 100 steps\n",
        ")\n",
        "\n",
        "# Load the pre-trained model\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"  # Replace with your model name if different\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Initialize the Trainer with `predict_with_generate` set in the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    predict_with_generate=True  # Set this inside the Trainer\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "vOkCtzrctQ50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./nllb_trained',  # Output directory for saving model checkpoints\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
        "    learning_rate=2e-5,  # Learning rate\n",
        "    per_device_train_batch_size=8,  # Batch size for training\n",
        "    per_device_eval_batch_size=8,  # Batch size for evaluation\n",
        "    num_train_epochs=3,  # Number of epochs\n",
        "    weight_decay=0.01,  # Weight decay for regularization\n",
        "    save_steps=500,  # Save model every 500 steps\n",
        "    logging_dir='./logs',  # Directory for logs\n",
        "    logging_steps=100,  # Log every 100 steps\n",
        "    predict_with_generate=True  # Use generate method for evaluation\n",
        ")\n"
      ],
      "metadata": {
        "id": "1--Lf6L8tEg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"Medical Translation.csv\")  # Adjust the path if necessary\n",
        "\n",
        "# Split dataset into train (80%), validation (10%), and test (10%)\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df[\"english\"].tolist(), df[\"tigrinya\"].tolist(), test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts, temp_labels, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_data = Dataset.from_dict({\"english\": train_texts, \"tigrinya\": train_labels})\n",
        "val_data = Dataset.from_dict({\"english\": val_texts, \"tigrinya\": val_labels})\n",
        "test_data = Dataset.from_dict({\"english\": test_texts, \"tigrinya\": test_labels})\n",
        "\n",
        "# Prepare the DatasetDict for tokenization\n",
        "datasets = DatasetDict({\n",
        "    \"train\": train_data,\n",
        "    \"validation\": val_data,\n",
        "    \"test\": test_data\n",
        "})\n"
      ],
      "metadata": {
        "id": "Zqes3fVIpdPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Ensure column names match exactly\n",
        "df['tokenized_english'] = df['english'].apply(lambda x: str(x).split('. '))\n",
        "df['tokenized_tigrinya'] = df['tigrinya'].apply(lambda x: str(x).split('። '))\n",
        "df[['tokenized_english', 'tokenized_tigrinya']].head()"
      ],
      "metadata": {
        "id": "AS7jeQQzOSqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "                **Check Sentence Length Distribution**\n",
        "\n",
        "We need to analyze the length of sentences to ensure they fit within the model's input constraints."
      ],
      "metadata": {
        "id": "DZVF_VRO2MnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check sentence length distribution\n",
        "df['eng_length'] = df['english'].apply(lambda x: len(str(x).split()))\n",
        "df['tir_length'] = df['tigrinya'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "df[['eng_length', 'tir_length']].describe()"
      ],
      "metadata": {
        "id": "PPbHNVbt2euZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "            **Filtering Extremely Long or Short Sentences**\n",
        "\n",
        "Very short or long sentences might reduce model performance. We filter out sentences that are too short (<3 words) or too long (>128 words)."
      ],
      "metadata": {
        "id": "QaiSvFjH4_MD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out sentences that are too short or too long\n",
        "df = df[(df['eng_length'] >= 3) & (df['eng_length'] <= 128)]\n",
        "df = df[(df['tir_length'] >= 3) &  (df['tir_length'] <= 128)]\n",
        "\n",
        "print(f\"Dataset size after filtering: {df.shape}\")"
      ],
      "metadata": {
        "id": "2q0HZ3UC-igl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)  # Verify column names"
      ],
      "metadata": {
        "id": "4myKXvBFunuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "               **Save the Processed Dataset**\n",
        "\n",
        "After preprocessing, we save the cleaned and tokenized dataset for use in model training."
      ],
      "metadata": {
        "id": "i1hH_5Hq_BJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"cleaned_Medical Translation.csv\", index=False, encoding='utf-8')\n",
        "print(\"Dataset saved successfully!\")"
      ],
      "metadata": {
        "id": "OexR0oG2_PTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())  # Shows current directory\n",
        "print(os.listdir())  # Lists all files in the directory\n"
      ],
      "metadata": {
        "id": "3qZXzfQ4C_YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_path = \"/content/AI_Tigrinya_Translation/cleaned_Medical Translation.csv\"\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    print(\"File exists:\", file_path)\n",
        "else:\n",
        "    print(\"File does NOT exist!\")\n"
      ],
      "metadata": {
        "id": "RDS-zycpU5H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get the current working directory\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "# List all files in the directory\n",
        "files = os.listdir(current_dir)\n",
        "\n",
        "# Print full paths\n",
        "for file in files:\n",
        "    full_path = os.path.join(current_dir, file)\n",
        "    print(full_path)"
      ],
      "metadata": {
        "id": "LVuMaCSkOHXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tokenized_tigrinya'] = df['tokenized_tigrinya'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)"
      ],
      "metadata": {
        "id": "O1m_f9Osp3I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"cleaned_dataset.csv\", index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "J77SJy_Qq1h2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['tokenized_english', 'tokenized_tigrinya']].head(10)"
      ],
      "metadata": {
        "id": "dyLsEq_zqPF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To download the cleaned dataset\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"cleaned_Medical Translation.csv\")"
      ],
      "metadata": {
        "id": "5_tKRcXiDbe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "      **Fine-Tuning NLLB-200 Model **\n",
        "\n",
        "      Now that we have a cleaned and tokenized dataset,\n",
        "      we can fine-tune the NLLB-200 model to improve translation\n",
        "      performance for English ⇆ Tigrinya in medical and legal contexts.\n",
        "\n",
        "      **Install and Load Hugging Face Transformers**\n",
        "\n",
        "Before using the model, install and import the necessary libraries:"
      ],
      "metadata": {
        "id": "knkWVZIwtLX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "dqz00p_3yz4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Pretrained NLLB-200 Model**\n",
        "\n",
        "We will use Facebook's NLLB-200 model, specifically the distilled 600M version, which is optimized for translation tasks."
      ],
      "metadata": {
        "id": "uabWQMzqz5zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "Las73mZvOK6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"cleaned_Medical Translation.csv\")\n",
        "print(df.head())  # Show first few rows\n",
        "print(df.columns)  # Display column names\n"
      ],
      "metadata": {
        "id": "1Qa_C7KwTHZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = df.columns.str.strip()  # Remove leading/trailing spaces\n",
        "df.rename(columns={\"english\": \"source\", \"tigrinya\": \"target\"}, inplace=True)\n"
      ],
      "metadata": {
        "id": "OKqL8F7tTQey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df[\"source\"].tolist(), df[\"target\"].tolist(), test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "5WxcqruoTj91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "UubNeIojSLLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"cleaned_Medical Translation.csv\")\n",
        "\n",
        "# Debug: Print column names\n",
        "print(\"Dataset Columns:\", df.columns)\n",
        "\n",
        "# Rename columns to match expected format\n",
        "df.rename(columns={\"english\": \"source\", \"tigrinya\": \"target\"}, inplace=True)\n",
        "\n",
        "# Split dataset into train (80%), validation (10%), and test (10%)\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df[\"source\"], df[\"target\"], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts, temp_labels, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_dict({\"source\": train_texts.tolist(), \"target\": train_labels.tolist()})\n",
        "val_dataset = Dataset.from_dict({\"source\": val_texts.tolist(), \"target\": val_labels.tolist()})\n",
        "test_dataset = Dataset.from_dict({\"source\": test_texts.tolist(), \"target\": test_labels.tolist()})\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": val_dataset,\n",
        "    \"test\": test_dataset,\n",
        "})\n",
        "\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "YF4g90l1Ugrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"  # Choose appropriate NLLB-200 variant\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer(examples[\"source\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(examples[\"target\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "print(\"Tokenization complete!\")\n"
      ],
      "metadata": {
        "id": "SYhjNgcAVSf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade transformers\n"
      ],
      "metadata": {
        "id": "GNBDx75uWLlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Define the model name (Use the actual model name you're working with)\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"  # Or another variant\n",
        "\n",
        "# Load Pretrained Model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "cDIxJsYQXrzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the dataset\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer(examples[\"english\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(examples[\"tigrinya\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_data = Dataset.from_dict({\"english\": train_texts, \"tigrinya\": train_labels})\n",
        "val_data = Dataset.from_dict({\"english\": val_texts, \"tigrinya\": val_labels})\n",
        "test_data = Dataset.from_dict({\"english\": test_texts, \"tigrinya\": test_labels})\n",
        "\n",
        "# Apply the tokenization function\n",
        "tokenized_datasets = DatasetDict({\n",
        "    \"train\": train_data.map(preprocess_function, batched=True),\n",
        "    \"validation\": val_data.map(preprocess_function, batched=True),\n",
        "    \"test\": test_data.map(preprocess_function, batched=True),\n",
        "})\n",
        "\n",
        "print(\"Tokenization complete!\")"
      ],
      "metadata": {
        "id": "_FKaJWHCZ8W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./nllb_trained\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,  # Use FP16 if on GPU\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=500,\n",
        ")\n"
      ],
      "metadata": {
        "id": "mHj-_TZinnyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Define the model name\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"  # Or the path to your fine-tuned model\n",
        "\n",
        "# Load Pretrained Model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, Trainer\n",
        "\n",
        "# Load Pretrained Model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "kuc004Bknyhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare Data for Fine-Tuning**\n",
        "\n",
        "We need to format our cleaned dataset to be compatible with the model."
      ],
      "metadata": {
        "id": "2GI7Zy0I04vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(examples[\"english\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    targets = tokenizer(examples[\"tigrinya\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "# Load cleaned dataset\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"cleaned_Medical Translation.csv\")\n",
        "\n",
        "# Convert to Hugging Face dataset format\n",
        "dataset = Dataset.from_pandas(df)\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "ysk8D0XE0_8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Training Arguments**\n",
        "\n",
        "We set up configurations for fine-tuning, such as batch size, learning rate, and evaluation strategy."
      ],
      "metadata": {
        "id": "uFJowBCK1p0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine-tune the Model**\n",
        "\n",
        "Now you can fine-tune the NLLB-200 model using the training arguments you defined. Make sure that you have your dataset loaded and preprocessed correctly."
      ],
      "metadata": {
        "id": "QCE97-rvKGSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "id": "v8q4618ZM52Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Load the pre-trained NLLB-200 model and tokenizer\n",
        "model_name = \"facebook/nllb-200-3.3B\"  # Replace with the correct NLLB model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "an0V_zjDNj8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "KXKMxTjoOQ7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls -la"
      ],
      "metadata": {
        "id": "I-le8vqvR6XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find . -name \".gitignore\""
      ],
      "metadata": {
        "id": "3igQmi3VTEuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find . -name \"cleaned_Medical Translation.CSV\""
      ],
      "metadata": {
        "id": "lZ5Drd5pSH2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls | grep \"cleaned_Medical Translation.csv\"\n"
      ],
      "metadata": {
        "id": "gzwaWq5_fV-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git ls-files | grep \"cleaned_Medical Translation.CSV\""
      ],
      "metadata": {
        "id": "bZATARVrTxWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git ls-files | grep \".env\""
      ],
      "metadata": {
        "id": "carbjbUoUAhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "# Define dataset path\n",
        "dataset_path = \"/content/AI_Tigrinya_Translation/cleaned_Medical Translation.csv\"\n",
        "\n",
        "# Load dataset into Pandas DataFrame\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Ensure dataset has correct columns\n",
        "source_column = \"english\"\n",
        "target_column = \"tigrinya\"\n",
        "\n",
        "if source_column not in df.columns or target_column not in df.columns:\n",
        "    raise ValueError(f\"Expected columns '{source_column}' and '{target_column}' not found in dataset!\")\n",
        "\n",
        "# Drop any rows with missing values (avoiding NoneType errors)\n",
        "df = df.dropna(subset=[source_column, target_column])\n",
        "\n",
        "# Split dataset (90% training, 10% evaluation)\n",
        "train_df, eval_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "eval_dataset = Dataset.from_pandas(eval_df)\n",
        "\n",
        "# Load NLLB-200 model and tokenizer\n",
        "model_name = \"facebook/nllb-200-3.3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# ✅ Tokenization function (updated)\n",
        "def tokenize_function(examples):\n",
        "    # Ensure no NoneType values exist\n",
        "    inputs = [str(text) for text in examples[source_column]]\n",
        "    targets = [str(text) for text in examples[target_column]]\n",
        "\n",
        "    # Tokenization\n",
        "    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True)\n",
        "    labels = tokenizer(targets, padding=\"max_length\", truncation=True, text_target=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# ✅ Apply tokenization\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# ✅ Remove unnecessary columns\n",
        "columns_to_keep = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "train_dataset.set_format(type=\"torch\", columns=columns_to_keep)\n",
        "eval_dataset.set_format(type=\"torch\", columns=columns_to_keep)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=AutoModelForSeq2SeqLM.from_pretrained(model_name),\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# ✅ Train the model\n",
        "trainer.train()\n",
        "\n",
        "# ✅ Evaluate the model\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "FAJ3kIkM0Bog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "# Define dataset path\n",
        "dataset_path = \"/content/AI_Tigrinya_Translation/cleaned_Medical Translation.csv\"\n",
        "\n",
        "# Load dataset into Pandas DataFrame\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Ensure dataset has correct columns\n",
        "source_column = \"english\"   # Update if different\n",
        "target_column = \"tigrinya\"  # Update if different\n",
        "\n",
        "if source_column not in df.columns or target_column not in df.columns:\n",
        "    raise ValueError(f\"Expected columns '{source_column}' and '{target_column}' not found in dataset!\")\n",
        "\n",
        "# Split dataset (90% training, 10% evaluation)\n",
        "train_df, eval_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "eval_dataset = Dataset.from_pandas(eval_df)\n",
        "\n",
        "# Load NLLB-200 model and tokenizer\n",
        "model_name = \"facebook/nllb-200-3.3B\"  # Update if needed\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# ✅ Tokenization function (sentence-level tokenization)\n",
        "def tokenize_function(examples):\n",
        "    model_inputs = tokenizer(examples[source_column], padding=\"max_length\", truncation=True)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[target_column], padding=\"max_length\", truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# ✅ Apply tokenization (ensuring 'input_ids' and 'attention_mask' exist)\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# ✅ Remove unnecessary columns (keep only required ones)\n",
        "columns_to_keep = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "train_dataset.set_format(type=\"torch\", columns=columns_to_keep)\n",
        "eval_dataset.set_format(type=\"torch\", columns=columns_to_keep)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=AutoModelForSeq2SeqLM.from_pretrained(model_name),\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# ✅ Train the model\n",
        "trainer.train()\n",
        "\n",
        "# ✅ Evaluate the model\n",
        "trainer.evaluate()\n"
      ],
      "metadata": {
        "id": "07LTXe6hzojv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the cleaned dataset\n",
        "dataset_path = \"/content/AI_Tigrinya_Translation/cleaned_Medical Translation.csv\"\n",
        "df = pd.read_csv(dataset_path).dropna()\n",
        "\n",
        "# Ensure necessary columns exist\n",
        "required_columns = {\"input_ids\", \"attention_mask\", \"tokenized_tigrinya\"}\n",
        "if not required_columns.issubset(df.columns):\n",
        "    raise ValueError(f\"Dataset is missing required columns: {required_columns - set(df.columns)}\")\n",
        "\n",
        "# Rename 'tokenized_tigrinya' to 'labels' (this is required for the Trainer)\n",
        "df = df.rename(columns={\"tokenized_tigrinya\": \"labels\"})\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df = df[[\"input_ids\", \"attention_mask\", \"labels\"]]\n",
        "\n",
        "# Convert string representations of lists into actual lists\n",
        "df[\"input_ids\"] = df[\"input_ids\"].apply(eval)\n",
        "df[\"attention_mask\"] = df[\"attention_mask\"].apply(eval)\n",
        "df[\"labels\"] = df[\"labels\"].apply(eval)\n",
        "\n",
        "# Split the dataset\n",
        "train_df, eval_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "eval_dataset = Dataset.from_pandas(eval_df)\n",
        "\n",
        "# Load pre-trained NLLB-200 model and tokenizer\n",
        "model_name = \"facebook/nllb-200-3.3B\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,  # Reduce batch size if OOM\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "trainer.evaluate()\n"
      ],
      "metadata": {
        "id": "GPH5ui0rzBQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the cleaned, already tokenized dataset\n",
        "dataset_path = \"/content/AI_Tigrinya_Translation/cleaned_Medical Translation.csv\"\n",
        "df = pd.read_csv(dataset_path).dropna()\n",
        "\n",
        "# Ensure all columns are present\n",
        "if \"english\" not in df.columns or \"tigrinya\" not in df.columns:\n",
        "    raise ValueError(\"Tokenized dataset must contain 'input_ids' and 'attention_mask' columns!\")\n",
        "\n",
        "# Split the dataset (90% training, 10% evaluation)\n",
        "train_df, eval_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert Pandas DataFrame to Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "eval_dataset = Dataset.from_pandas(eval_df)\n",
        "\n",
        "# Load pre-trained NLLB-200 model and tokenizer\n",
        "model_name = \"facebook/nllb-200-3.3B\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,  # Reduce batch size if OOM\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "RWPXHAxUtcXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "# Load dataset\n",
        "dataset_path = \"/content/AI_Tigrinya_Translation/cleaned_Medical Translation.csv\"\n",
        "df = pd.read_csv(dataset_path).dropna()  # Drop missing values\n",
        "\n",
        "# Ensure dataset has correct columns\n",
        "source_column = \"english\"\n",
        "target_column = \"tigrinya\"\n",
        "\n",
        "if source_column not in df.columns or target_column not in df.columns:\n",
        "    raise ValueError(f\"Expected columns '{source_column}' and '{target_column}' not found in dataset!\")\n",
        "\n",
        "# Convert everything to string\n",
        "df[source_column] = df[source_column].astype(str)\n",
        "df[target_column] = df[target_column].astype(str)\n",
        "\n",
        "# Split the dataset (90% training, 10% evaluation)\n",
        "train_df, eval_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "eval_dataset = Dataset.from_pandas(eval_df)\n",
        "\n",
        "# Load the pre-trained NLLB-200 model and tokenizer\n",
        "model_name = \"facebook/nllb-200-3.3B\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        [str(text) for text in examples[source_column]],\n",
        "        text_target=[str(text) for text in examples[target_column]],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "# Apply tokenization\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,  # Reduce batch size if OOM error occurs\n",
        "    per_device_eval_batch_size=4,   # Reduce batch size if OOM error occurs\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "trainer.evaluate()\n"
      ],
      "metadata": {
        "id": "kBvnCHGutN48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "# Load dataset\n",
        "dataset_path = \"/content/AI_Tigrinya_Translation/cleaned_Medical Translation.csv\"\n",
        "df = pd.read_csv(dataset_path).dropna()  # Remove missing values\n",
        "\n",
        "# Ensure dataset has correct columns\n",
        "source_column = \"english\"\n",
        "target_column = \"tigrinya\"\n",
        "\n",
        "if source_column not in df.columns or target_column not in df.columns:\n",
        "    raise ValueError(f\"Expected columns '{source_column}' and '{target_column}' not found in dataset!\")\n",
        "\n",
        "# Convert everything to string\n",
        "df[source_column] = df[source_column].astype(str)\n",
        "df[target_column] = df[target_column].astype(str)\n",
        "\n",
        "# Split the dataset (90% training, 10% evaluation)\n",
        "train_df, eval_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "eval_dataset = Dataset.from_pandas(eval_df)\n",
        "\n",
        "# Load the pre-trained NLLB-200 model and tokenizer\n",
        "model_name = \"facebook/nllb-200-3.3B\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "# Sentence-based tokenization function\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize sentences individually\n",
        "    inputs = [sentence.strip() for sentence in examples[source_column]]\n",
        "    targets = [sentence.strip() for sentence in examples[target_column]]\n",
        "\n",
        "    return tokenizer(\n",
        "        inputs,\n",
        "        text_target=targets,\n",
        "        padding=\"max_length\",  # Ensures fixed input length\n",
        "        truncation=True,  # Prevents exceeding max length\n",
        "        max_length=128  # Adjust based on sentence length\n",
        "    )\n",
        "\n",
        "# Apply tokenization sentence-wise\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,  # Adjust if OOM occurs\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "trainer.evaluate()\n"
      ],
      "metadata": {
        "id": "CHeHH3MbsqcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "# Define paths\n",
        "dataset_path = \"/content/AI_Tigrinya_Translation/cleaned_Medical Translation.csv\"\n",
        "\n",
        "# Load dataset into a Pandas DataFrame\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Ensure dataset has correct columns\n",
        "source_column = \"english\"  # Update if different\n",
        "target_column = \"tigrinya\"  # Update if different\n",
        "\n",
        "if source_column not in df.columns or target_column not in df.columns:\n",
        "    raise ValueError(f\"Expected columns '{source_column}' and '{target_column}' not found in dataset!\")\n",
        "\n",
        "# Split the dataset (90% training, 10% evaluation)\n",
        "train_df, eval_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "eval_dataset = Dataset.from_pandas(eval_df)\n",
        "\n",
        "# Load the pre-trained NLLB-200 model and tokenizer\n",
        "model_name = \"facebook/nllb-200-3.3B\"  # Update with the correct model if needed\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[source_column], text_target=examples[target_column],\n",
        "                     padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Apply tokenization\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "dapc2J-0geiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the pre-trained NLLB-200 model and tokenizer\n",
        "model_name = \"facebook/nllb-200-3.3B\"  # Replace with the correct NLLB model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load your custom dataset (ensure it's properly tokenized and preprocessed)\n",
        "# Example: Load dataset for training and evaluation (you can update the path if needed)\n",
        "train_dataset = load_dataset('/content/AI_Tigrinya_Translation/cleaned_Medical Translation.csv')  # Update with actual data path\n",
        "eval_dataset = load_dataset('/content/AI_Tigrinya_Translation/cleaned_Medical Translation.csv')    # Update with actual data path\n",
        "\n",
        "# Tokenize the datasets\n",
        "def tokenize_function(examples):\n",
        "    # Assuming the dataset has columns 'source_text' and 'target_text'\n",
        "    return tokenizer(examples['source_text'], text_target=examples['target_text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',              # output directory for model checkpoints\n",
        "    evaluation_strategy=\"epoch\",         # evaluate the model at the end of each epoch\n",
        "    save_strategy=\"epoch\",               # save model checkpoint at the end of each epoch (to match eval_strategy)\n",
        "    learning_rate=2e-5,                  # learning rate for the optimizer\n",
        "    per_device_train_batch_size=8,       # batch size for training\n",
        "    per_device_eval_batch_size=8,        # batch size for evaluation\n",
        "    num_train_epochs=3,                  # number of training epochs\n",
        "    weight_decay=0.01,                   # weight decay to avoid overfitting\n",
        "    logging_dir='./logs',                # directory for storing logs\n",
        "    logging_steps=500,                   # log every 500 steps\n",
        "    save_steps=500,                      # save model checkpoint every 500 steps\n",
        "    save_total_limit=2,                  # maximum number of checkpoints to save\n",
        "    load_best_model_at_end=True,         # load the best model when finished training\n",
        "    metric_for_best_model=\"accuracy\",   # use accuracy for model selection\n",
        "    push_to_hub=False                    # set to True to push model to Hugging Face Hub\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with the training arguments, model, and dataset\n",
        "trainer = Trainer(\n",
        "    model=model,                        # the model to be fine-tuned\n",
        "    args=training_args,                 # the training arguments defined earlier\n",
        "    train_dataset=train_dataset,        # the training dataset\n",
        "    eval_dataset=eval_dataset,          # the evaluation dataset\n",
        "    tokenizer=tokenizer                 # the tokenizer used to encode/decode text\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "A_rdg1HVP6fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "6XW2GfOStcAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Pretrained NLLB-200 Model"
      ],
      "metadata": {
        "id": "Bu4hfh8AvKtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "WaMgA6ezvMPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Data for Fine-Tuning"
      ],
      "metadata": {
        "id": "AJofj-RWv2p4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(examples[\"english\"], max_length=128, truncation=True)\n",
        "    targets = tokenizer(examples[\"tigrinya\"], max_length=128, truncation=True)\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "dataset =Dataset.from_pandas(df)\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "PClBNLSRv-KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "        ** Train the Model**"
      ],
      "metadata": {
        "id": "MflRaA5jwkyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "Re5Fn5Gfyc6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply train_test_split to the \"train\" dataset\n",
        "dataset_split = dataset[\"train\"].train_test_split(test_size=0.1)\n",
        "\n",
        "# Create a new DatasetDict that includes both train and eval sets\n",
        "dataset = {\n",
        "    \"train\": dataset_split[\"train\"],\n",
        "    \"test\": dataset_split[\"test\"],  # This will be used as eval_dataset\n",
        "}\n",
        "\n",
        "# Assign train and eval sets\n",
        "train_data = dataset[\"train\"]\n",
        "eval_data = dataset[\"test\"]\n",
        "\n",
        "print(train_data)\n",
        "print(eval_data)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=eval_data,  # Now eval_dataset is correctly provided\n",
        ")\n"
      ],
      "metadata": {
        "id": "hTCgKYvu2SBx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}